{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#all_wip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Introduction\n",
    "\n",
    "Hey all, this competition is going to be an interesting one given the nature of the dataset. It's not a straighforward image classification problem and one can forulate the problem statement in multiple ways.\n",
    "\n",
    "In my [EDA kernel](http://wandb.me/brain-eda), I have shown that the MRI images per patient and scan type is sequential in nature. \n",
    "\n",
    "As a doctor I would be interested to look at multiple slices (images) of the MRI Sequence to determine if a patient has brain tumor or not. Thus I have formulated the brain tumor classification problem statement as Video Classification. \n",
    "\n",
    "üôè This is a work in progress and I would love to hear your suggestions to improve the training pipeline. \n",
    "\n",
    "I am using the dataset created by [Jonathan Besomi](https://www.kaggle.com/jonathanbesomi). Many thanks to him for creating this. You can find the data [here](https://www.kaggle.com/jonathanbesomi/rsna-miccai-png). \n",
    "\n",
    "I have implemented the pipeline in TensorFlow and using [Weights and Biases](https://wandb.ai/site) for experiment tracking and data visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî≠ Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "#export\r\n",
    "import os\r\n",
    "import re\r\n",
    "import gc\r\n",
    "import glob\r\n",
    "import imageio\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "print('TF version: ', tf.__version__)\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "from tensorflow.keras.models import *\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B version:  0.10.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\r\n",
    "import wandb\r\n",
    "print('W&B version: ', wandb.__version__)\r\n",
    "from wandb.keras import WandbCallback\r\n",
    "\r\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#export\r\n",
    "gpus = tf.config.list_physical_devices('GPU')\r\n",
    "if gpus:\r\n",
    "  try:\r\n",
    "    # Currently, memory growth needs to be the same across GPUs\r\n",
    "    for gpu in gpus:\r\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\r\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n",
    "  except RuntimeError as e:\r\n",
    "    # Memory growth must be set before GPUs have been initialized\r\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåà Prepare Dataset\n",
    "\n",
    "There are four sub-directories per patient corresponding to different MRI Image Sequencing methods. In this kernel, I am using \"FLAIR\" MRI to get the balls rolling. To get the maximum out of the dataset using every sequencing method is recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS21ID</th>\n",
       "      <th>MGMT_value</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/rsna-miccai-png/train/00000/FLAIR/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/rsna-miccai-png/train/00002/FLAIR/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/rsna-miccai-png/train/00003/FLAIR/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/rsna-miccai-png/train/00005/FLAIR/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/rsna-miccai-png/train/00006/FLAIR/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BraTS21ID  MGMT_value                                         path\n",
       "0          0           1  ../input/rsna-miccai-png/train/00000/FLAIR/\n",
       "1          2           1  ../input/rsna-miccai-png/train/00002/FLAIR/\n",
       "2          3           0  ../input/rsna-miccai-png/train/00003/FLAIR/\n",
       "3          5           1  ../input/rsna-miccai-png/train/00005/FLAIR/\n",
       "4          6           1  ../input/rsna-miccai-png/train/00006/FLAIR/"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training csv file\r\n",
    "df = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\r\n",
    "\r\n",
    "def get_patient_id(patient_id):\r\n",
    "    if patient_id < 10:\r\n",
    "        return '0000'+str(patient_id)\r\n",
    "    elif patient_id >= 10 and patient_id < 100:\r\n",
    "        return '000'+str(patient_id)\r\n",
    "    elif patient_id >= 100 and patient_id < 1000:\r\n",
    "        return '00'+str(patient_id)\r\n",
    "    else:\r\n",
    "        return '0'+str(patient_id)\r\n",
    "\r\n",
    "def get_path(row):\r\n",
    "    patient_id = get_patient_id(row.BraTS21ID)\r\n",
    "    return f'../input/rsna-miccai-png/train/{patient_id}/FLAIR/'\r\n",
    "\r\n",
    "df['path'] = df.apply(lambda row: get_path(row), axis=1)\r\n",
    "\r\n",
    "# Removing two patient ids from the dataframe since there are not FLAIR directories for these ids. \r\n",
    "df = df.loc[df.BraTS21ID!=109]\r\n",
    "df = df.loc[df.BraTS21ID!=709]\r\n",
    "df = df.reset_index(drop=True)\r\n",
    "\r\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train-test split. Note that there are only 585 patients so if you are doing video classification, K-fold training might be beneficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_df: 524; valid_df: 59\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=0.1, stratify=df.MGMT_value.values)\r\n",
    "print(f'Size of train_df: {len(train_df)}; valid_df: {len(valid_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\r\n",
    "    NUM_FRAMES = 10,\r\n",
    "    BATCH_SIZE = 8,\r\n",
    "    EPOCHS = 100,\r\n",
    "    IMG_SIZE = 224,\r\n",
    "    LSTM_UNITS = 256,\r\n",
    "    competition = 'rsna-miccai-brain',\r\n",
    "    _wandb_kernel = 'ayut'\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Video Classification Data Pipeline\n",
    "\n",
    "A video classification data pipeline will compromise of multiple frames of the same video batched together. In order to batch the frames, the number of frames should be same. In this kerel it's controlled by `NUM_FRAMES`. \n",
    "\n",
    "In have implemented the data pipeline using purely `tf.data`. Here are the important points to note:\n",
    "\n",
    "* The images in each `patient_id/FLAIR` directory is listed down using `glob.glob`. <br>\n",
    "* The path to images need to be sorted as per the image id given by `Image-X.png`. This is done by `sorted_nicely` function below. <br>\n",
    "* We need to select a window of frames given by `NUM_FRAMES`. I am using uniform sampling to do so. One can device a better sampling method. <br>\n",
    "* Iterate through each frame (image), load them and resize them. üöÄ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/2669120/7636462\r\n",
    "def sorted_nicely(l): \r\n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \r\n",
    "    convert = lambda text: int(text) if text.isdigit() else text \r\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \r\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    # Normalize image\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def parse_frames(dirname):\n",
    "    # get MRI images file paths for given patient \n",
    "    paths = glob.glob(dirname.decode('utf8')+'/*.png')\n",
    "    # Sort the images to get sequential imaging\n",
    "    paths = sorted_nicely(paths)\n",
    "    \n",
    "    # randomly select a window of images to be used as sequence\n",
    "    start = tf.random.uniform((1,), maxval=len(paths)-CONFIG['NUM_FRAMES'], dtype=tf.int32)\n",
    "\n",
    "    paths = tf.slice(paths, start, [CONFIG['NUM_FRAMES']])\n",
    "    \n",
    "    def get_frames(path):\n",
    "        # Load image\n",
    "        image = tf.io.read_file(path)\n",
    "        image = decode_image(image)\n",
    "        # Resize image\n",
    "        image = tf.image.resize(image, (CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']))\n",
    "        \n",
    "        return image\n",
    "\n",
    "    mri_images = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn=get_frames, elems=paths, fn_output_signature=tf.float32))\n",
    "    \n",
    "    return mri_images\n",
    "    \n",
    "def load_frame(df_dict):\n",
    "    dirname = df_dict['path']\n",
    "    paths = tf.numpy_function(parse_frames, [dirname], tf.float32)\n",
    "    \n",
    "    # Parse label\n",
    "    label = df_dict['MGMT_value']\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    \n",
    "    return paths, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "trainloader = tf.data.Dataset.from_tensor_slices(dict(train_df))\n",
    "validloader = tf.data.Dataset.from_tensor_slices(dict(valid_df))\n",
    "\n",
    "\n",
    "trainloader = (\n",
    "    trainloader\n",
    "    .shuffle(1024)\n",
    "    .map(load_frame, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(CONFIG['BATCH_SIZE'])\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "validloader = (\n",
    "    validloader\n",
    "    .map(load_frame, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(CONFIG['BATCH_SIZE'])\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out the trainloader\n",
    "frames, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the samples from our trainloader, I am using W&B. I find it easier to log everything onto W&B to visualize data than to write Matplotlib code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worldly-glitter-28</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ayush-thakur/brain-tumor-video\" target=\"_blank\">https://wandb.ai/ayush-thakur/brain-tumor-video</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ayush-thakur/brain-tumor-video/runs/35hz1rkl\" target=\"_blank\">https://wandb.ai/ayush-thakur/brain-tumor-video/runs/35hz1rkl</a><br/>\n",
       "                Run data is saved locally in <code>/kaggle/working/wandb/run-20210721_220833-35hz1rkl</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1195<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 1.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.00055185066‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/kaggle/working/wandb/run-20210721_220833-35hz1rkl/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/kaggle/working/wandb/run-20210721_220833-35hz1rkl/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>7</td></tr><tr><td>_timestamp</td><td>1626905320</td></tr><tr><td>_step</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>‚ñÅ</td></tr><tr><td>_timestamp</td><td>‚ñÅ</td></tr><tr><td>_step</td><td>‚ñÅ</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 8 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">worldly-glitter-28</strong>: <a href=\"https://wandb.ai/ayush-thakur/brain-tumor-video/runs/35hz1rkl\" target=\"_blank\">https://wandb.ai/ayush-thakur/brain-tumor-video/runs/35hz1rkl</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='brain-tumor-video', job_type='dataloader-viz')\n",
    "\n",
    "os.makedirs('gifs/')\n",
    "for i, frame in enumerate(frames):\n",
    "    imageio.mimsave(f'gifs/out_{i}.gif', (frame*255).numpy().astype('uint8'))    \n",
    "\n",
    "wandb.log({'examples': [wandb.Image(f'gifs/out_{i}.gif', caption=f'{label.numpy()}') for i, label in enumerate(labels)]})\n",
    "    \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRI Sequences, where each sequence is `NUM_FRAMES` long. \n",
    "\n",
    "![img](https://i.imgur.com/pKc7rnT.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöú Model\n",
    "\n",
    "In order to model both spatial and temporal nature of videos, we can use a hybrid of CNN + LSTM model. \n",
    "\n",
    "* The `FeatureExtractor` model uses an EfficientNetB0 model as CNN backbone. It will be used to model the spatial aspect of videos. <br>\n",
    "* The `MRIModel` uses a `TimeDistributed` layer that runs the `FeatureExtractor` `NUM_FRAMES` times to get a vector of `(NUM_FRAMES, 1280)`. <br>\n",
    "* This is then fed to a single LSTM layer. You can use GRU and even Transformer in place of LSTM. I have used 256 units as it gave me the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 3)       30        \n",
      "_________________________________________________________________\n",
      "efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "=================================================================\n",
      "Total params: 4,049,601\n",
      "Trainable params: 4,007,578\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def FeatureExtractor():\n",
    "    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n",
    "    base_model.trainabe = True\n",
    "\n",
    "    inputs = Input((CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'], 1))\n",
    "    x = Conv2D(3, kernel_size=(3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = base_model(x, training=True)\n",
    "    flattened_output = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    return Model(inputs, flattened_output)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = FeatureExtractor()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10, 224, 224, 1)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 1280)          4049601   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 256)           1573888   \n",
      "_________________________________________________________________\n",
      "lstm_sigmoid (Dense)         (None, 10, 1)             257       \n",
      "=================================================================\n",
      "Total params: 5,623,746\n",
      "Trainable params: 5,581,723\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def MRIModel():\n",
    "    inputs = Input((CONFIG['NUM_FRAMES'], CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'], 1))\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    \n",
    "    time_wrapper = TimeDistributed(feature_extractor)(inputs)\n",
    "    \n",
    "    lstm_out = LSTM(CONFIG['LSTM_UNITS'], return_sequences=True, name=\"lstm\")(time_wrapper)\n",
    "    outputs = Dense(1, activation='sigmoid', name=\"lstm_sigmoid\")(lstm_out)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "tf.keras.backend.clear_session() \n",
    "model = MRIModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÖ Train\n",
    "\n",
    "This is a simple training pipeline that uses early stopping as regularizer and `WandbCallback` to log the metrics to Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "earlystopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='min',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">hardy-elevator-29</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ayush-thakur/brain-tumor-video\" target=\"_blank\">https://wandb.ai/ayush-thakur/brain-tumor-video</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ayush-thakur/brain-tumor-video/runs/1y2kh2qt\" target=\"_blank\">https://wandb.ai/ayush-thakur/brain-tumor-video/runs/1y2kh2qt</a><br/>\n",
       "                Run data is saved locally in <code>/kaggle/working/wandb/run-20210721_222246-1y2kh2qt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/66 [==============================] - 92s 1s/step - loss: 0.7761 - acc: 0.4861 - val_loss: 0.7122 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 63s 945ms/step - loss: 0.7013 - acc: 0.5174 - val_loss: 0.6994 - val_acc: 0.4881\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 61s 925ms/step - loss: 0.6993 - acc: 0.4932 - val_loss: 0.6985 - val_acc: 0.5119\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 65s 975ms/step - loss: 0.6951 - acc: 0.5142 - val_loss: 0.6841 - val_acc: 0.5305\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 63s 946ms/step - loss: 0.6890 - acc: 0.5576 - val_loss: 0.6878 - val_acc: 0.5220\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 59s 894ms/step - loss: 0.6911 - acc: 0.5363 - val_loss: 0.7056 - val_acc: 0.4695\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 58s 875ms/step - loss: 0.6766 - acc: 0.5938 - val_loss: 0.6829 - val_acc: 0.5831\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 58s 868ms/step - loss: 0.6820 - acc: 0.5782 - val_loss: 0.7002 - val_acc: 0.4983\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 57s 862ms/step - loss: 0.6898 - acc: 0.5421 - val_loss: 0.7036 - val_acc: 0.4678\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 57s 863ms/step - loss: 0.6844 - acc: 0.5579 - val_loss: 0.6872 - val_acc: 0.5814\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 57s 859ms/step - loss: 0.7001 - acc: 0.5442 - val_loss: 0.6975 - val_acc: 0.4881\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 56s 845ms/step - loss: 0.7014 - acc: 0.4936 - val_loss: 0.6753 - val_acc: 0.5932\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 56s 850ms/step - loss: 0.6803 - acc: 0.5767 - val_loss: 0.7113 - val_acc: 0.4085\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 60s 911ms/step - loss: 0.6928 - acc: 0.5040 - val_loss: 0.6852 - val_acc: 0.5441\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 56s 844ms/step - loss: 0.6917 - acc: 0.5200 - val_loss: 0.6802 - val_acc: 0.5831\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 56s 848ms/step - loss: 0.6882 - acc: 0.5419 - val_loss: 0.6958 - val_acc: 0.5373\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 56s 851ms/step - loss: 0.6870 - acc: 0.5296 - val_loss: 0.6797 - val_acc: 0.5695\n",
      "8/8 [==============================] - 3s 326ms/step - loss: 0.6778 - acc: 0.6068\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1778<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 64.90MB of 64.90MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/kaggle/working/wandb/run-20210721_222246-1y2kh2qt/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/kaggle/working/wandb/run-20210721_222246-1y2kh2qt/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>16</td></tr><tr><td>loss</td><td>0.68594</td></tr><tr><td>acc</td><td>0.54561</td></tr><tr><td>val_loss</td><td>0.67968</td></tr><tr><td>val_acc</td><td>0.56949</td></tr><tr><td>_runtime</td><td>1045</td></tr><tr><td>_timestamp</td><td>1626907212</td></tr><tr><td>_step</td><td>17</td></tr><tr><td>best_val_loss</td><td>0.67529</td></tr><tr><td>best_epoch</td><td>11</td></tr><tr><td>Val Accuracy</td><td>0.607</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>acc</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÖ‚ñÇ</td></tr><tr><td>val_acc</td><td>‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñà‚ñÅ‚ñÜ‚ñà‚ñÜ‚ñá</td></tr><tr><td>_runtime</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_timestamp</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Val Accuracy</td><td>‚ñÅ</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">hardy-elevator-29</strong>: <a href=\"https://wandb.ai/ayush-thakur/brain-tumor-video/runs/1y2kh2qt\" target=\"_blank\">https://wandb.ai/ayush-thakur/brain-tumor-video/runs/1y2kh2qt</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() \n",
    "model = MRIModel()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "run = wandb.init(project='brain-tumor-video', \n",
    "                 group='EffnetB0-LSTM-256', \n",
    "                 job_type='train', \n",
    "                 config=CONFIG)\n",
    "\n",
    "# Train\n",
    "_ = model.fit(trainloader, \n",
    "              epochs=CONFIG['EPOCHS'],\n",
    "              validation_data=validloader,\n",
    "              callbacks=[WandbCallback(),\n",
    "                         earlystopper])\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(validloader)\n",
    "wandb.log({'Val Accuracy': round(acc, 3)})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Check out the W&B Dashboard $\\rightarrow$](https://wandb.ai/ayush-thakur/brain-tumor-video?workspace=user-ayush-thakur)\n",
    "\n",
    "![img](https://i.imgur.com/GyP8XNR.gif)\n",
    "\n",
    "I have tried out three different LSTM units - 128, 256 and 512. \n",
    "\n",
    "You can see that the training curve is not stable. This kernel is a work in progress but wanted to share the idea with wider audience and see if it's a feasible idea to pursue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK IN PROGRESS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
